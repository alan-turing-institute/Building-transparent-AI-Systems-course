{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use use the Microblog Opinion Summarisation corpus described in https://arxiv.org/pdf/2208.04083.pdf ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: packaging in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: aiohttp in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: xxhash in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.10.0)\n",
      "Requirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.23.3)\n",
      "Requirement already satisfied: torchvision in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: scipy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (4.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.23.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (2022.9.13)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=24f9adb9fb097a0d503eb705233a3899dd6d9e50593bcaf4d5b97ed993d26365\n",
      "  Stored in directory: /Users/ibilal/Library/Caches/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-1.2.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install -r rouge/requirements.txt\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from transformers import BartForConditionalGeneration, AutoModel, AutoTokenizer, DataCollatorForSeq2Seq \n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sequence-to-sequence models have input limit of 512/1024 characters.\n",
    "\n",
    "Order the tweets within a cluster by relevance. \n",
    "We use sentence-tranformers library from Hugging Face to embed the tweets in a vector space and then use the mean cosine similarity as proxy for a tweet's relevance in the cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distance(tweet_emb):\n",
    "    distances = [1-util.pytorch_cos_sim(tweet_emb, tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
    "    return sum(distances)\n",
    "\n",
    "def cluster_distance2(tweet_emb):\n",
    "    distances = [numpy.linalg.norm(tweet_emb- tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
    "    return sum(distances)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our preprocessing, we remove emojis, URL placeholders and whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!😄 \n",
      "What a great day to learn! Check this URL_LINK\n",
      "['Hello World!  What a great day to learn! Check this ']\n"
     ]
    }
   ],
   "source": [
    "#Preprocess tweets\n",
    "def preprocess_(tweets):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    tweets = [emoji_pattern.sub(r'', tweet) for tweet in tweets]\n",
    "    tweets = [tweet.replace('\\n', ' ') for tweet in tweets]\n",
    "    tweets = [tweet.replace('\\r', ' ') for tweet in tweets]\n",
    "    tweets = [tweet.replace('\\t', ' ') for tweet in tweets]\n",
    "    tweets = [tweet.replace('URL_LINK','') for tweet in tweets]\n",
    "   \n",
    "    return tweets\n",
    "\n",
    "tweet = 'Hello World!😄 \\nWhat a great day to learn! Check this URL_LINK'\n",
    "print(tweet)\n",
    "print(preprocess_([tweet]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant 7 tweets in the cluster are: ['.@Susie_Wolff has stopped on the track at Silverstone. She is reporting a problem with oil pressure  #bbcf1', \",@Susie_Wolff stopped on track after four laps with oil pressure problem. Here's Claire Williams on Susie \", \"Susie #Wolff's car is stopped on the side of the track in the last sector, reporting lost oil pressure...#F1 #BritishGP\", 'MT “@bbcf1: @Susie_Wolff stopped on the track at Silverstone. She is reporting problem with oil pressure  #bbcf1” :-(', \"Gutted for @Susie_Wolff - car stopped out on track with oil pressure failure. 22 mins in she won't be able to run #F1 \", 'Just as I tweet its great to see out on track then @Susie_Wolff has oil pressure problems :(', \"Susie Wolff has stopped on track. Reporting that it's an oil pressure issue. Looks like the end of her session with 1:08.36 to go. #f1\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 7\n",
    "model_st = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def sort_relevance(cluster):\n",
    "    tweets = []\n",
    "    minv = 100000 \n",
    "    pp_tweets = preprocess_(cluster['tweets'])\n",
    "    tweet_embeddings = model_st.encode(pp_tweets)\n",
    "\n",
    "    for i, tweet in enumerate(pp_tweets):\n",
    "        tweets.append({'tweet':tweet, 'score': cluster_distance(tweet_embeddings[i])})\n",
    "\n",
    "    tweets = sorted(tweets, key=lambda x: x['score'])\n",
    "    return [x['tweet'] for x in tweets]\n",
    "\n",
    "with open('/Users/ibilal/Desktop/Research/Opinion_Text_Summarisation/testing_corpus/opi/2014-07-04-09_oil_0') as f:\n",
    "    cluster = json.load(f)\n",
    "print('The most relevant {} tweets in the cluster are:'.format(k), sort_relevance(cluster)[:k])    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Generation\n",
    "\n",
    "We work with Transformer-based model BART for the summarisation task. Its paper can be found here https://arxiv.org/pdf/1910.13461.pdf \n",
    "The model produces competitive results across many tasks and domains and it is especially effective for fine-tuning strategies.\n",
    "We follow the implementation from HuggingFace library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opinionated summaries are 37.8859649122807 tokens long on avg\n",
      "Non-Opinionated summaries are 18.41 tokens long on avg\n"
     ]
    }
   ],
   "source": [
    "# The final summary is the concatenation of the components: main story, majority opinion (if any) and any minority opinions (if any). \n",
    "def summary_concat(cluster):\n",
    "    opinionated = 1\n",
    "    if 'majority_opinion' in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
    "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion'] + ' ' + cluster['minority_opinions']\n",
    "    elif 'majority_opinion' in cluster.keys() and 'minority_opinions' not in cluster.keys():  \n",
    "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion']\n",
    "    elif 'majority_opinion' not in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
    "        summary = cluster['main_story'] + ' ' + cluster['minority_opinions']   \n",
    "    else:\n",
    "       summary = cluster['main_story']  \n",
    "       opinionated = 0\n",
    "    return summary, opinionated        \n",
    "\n",
    "\n",
    "avg_len_opi = 0\n",
    "avg_len_nopi = 0\n",
    "for file in os.listdir('/Users/ibilal/Desktop/Research/Code/mos_workspace/Partition_10'):\n",
    "    if not os.path.isfile(os.path.join('/Users/ibilal/Desktop/Research/Code/mos_workspace/Partition_10',file)):\n",
    "        continue\n",
    "    with open(os.path.join('/Users/ibilal/Desktop/Research/Code/mos_workspace/Partition_10',file),'r') as f:\n",
    "        cluster = json.load(f)\n",
    "    if summary_concat(cluster)[1] == 0:\n",
    "        avg_len_nopi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
    "    \n",
    "    else:\n",
    "        avg_len_opi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
    "\n",
    "avg_len_opi = avg_len_opi/len(os.listdir('/Users/ibilal/Desktop/Research/Code/mos_workspace/Partition_10/opinionated'))\n",
    "avg_len_nopi = avg_len_nopi/len(os.listdir('/Users/ibilal/Desktop/Research/Code/mos_workspace/Partition_10/non_opinionated'))\n",
    "\n",
    "print('Opinionated summaries are {:2} tokens long on avg'.format(avg_len_opi))\n",
    "print('Non-Opinionated summaries are {:2} tokens long on avg'.format(avg_len_nopi))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susie Wolff's Williams has come to a stop after four laps. Williams say the stoppage is not #Wolff's fault, she is reporting oil pressure problems. Massa has just hit the wall\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/bart-large-cnn'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def bart_zero(cluster):\n",
    "    original_text = ' '.join(sort_relevance(cluster))\n",
    "    inputs = tokenizer.encode(original_text, return_tensors=\"pt\", max_length=1024)\n",
    "    outputs = model.generate(inputs, max_length=46, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True,no_repeat_ngram_size=4)\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    return summary\n",
    "print(bart_zero(cluster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-tuning\n",
    "\n",
    "BART is fine-tuned for 5 epochs on the 10% partition of the MOS corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"text\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='/Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "datafiles ='/Users/ibilal/Desktop/Research/Code/mos_workspace/partition10.json'\n",
    "dataset = load_dataset('json', data_files=datafiles, split='train')\n",
    "tokenized_dataset = dataset.map(preprocess_function,batched=False)\n",
    "print(tokenized_dataset[0])\n",
    "print(len(tokenized_dataset))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=tokenized_dataset,         # training dataset\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Evaluation\n",
    "\n",
    "Generated summaries are evaluated against two dimensions:\n",
    "\n",
    "<b>Word-overlap</b>: ROUGE (Lin et al.,2004, https://aclanthology.org/W04-1013.pdf) is the most used metric in summarisation tasks. It calculates the n-gram overlap between system and candidate summaries. In our experiment, we use the harmonic mean F_1 of ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-L (longest sequence). By design, the metric is unable to detect semantic similarity (<i>happy kid</i> and <i>joyful child</i> are identified as disjoint and are penalised).\n",
    "\n",
    "<b>Semantic Similarity</b>: BERTScore (Zhang et al, 2020, https://arxiv.org/pdf/1904.09675.pdf) is used in machine generation tasks to detect the level of semantic similarity between a pair of texts. It relies on BERT contextual embeddings and greedy token matching and it has been shown to correlate well with human judgement. It achieved competitive results in machine translation and image captioning due to its robustness in paraphrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Evaluation\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
    "testing_set_path = '/Users/ibilal/Desktop/Research/Code/mos_workspace/testing_set'\n",
    "\n",
    "for partition in ['nopi_covid','nopi_elections','opi_covid','opi_elections']:\n",
    "    rouge1_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
    "    rouge2_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
    "    rougel_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
    "    bertscore_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0} \n",
    "    \n",
    "    for file in os.listdir(os.path.join(testing_set_path,partition)):\n",
    "        with open(os.path.join(testing_set_path,partition,file),'r') as f:\n",
    "            cluster = json.load(f)\n",
    "        gold_standard = summary_concat(cluster)[0]\n",
    "        \n",
    "        #Generate the summary candidates\n",
    "        medoid_summary = sort_relevance(cluster)[0]\n",
    "        bart_zero_summary = bart_zero(cluster)\n",
    "        summaries = {'medoid':medoid_summary, 'bart_zero':bart_zero_summary}\n",
    "        #For each summary type, compute its scores vs the human-written summary\n",
    "        for key in summaries.keys():\n",
    "        \n",
    "            scores = scorer.score(gold_standard, summaries[key])\n",
    "\n",
    "            rouge1_score[key] += scores['rouge1'][2]\n",
    "            rouge2_score[key] += scores['rouge2'][2]\n",
    "            rougel_score[key] += scores['rougel'][2]\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4f355363accec33f6aef60bd5e6eb3b3e1c068e14064fe89a75a68e833dd334"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('mos_jupyter': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
