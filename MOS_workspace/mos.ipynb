{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing models for the Task of Microblog Opinion Summarisation\n",
        "----------"
      ],
      "metadata": {
        "id": "PGGvuJqQkotL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klJ4ampNOc9W"
      },
      "source": [
        "We will use use the Microblog Opinion Summarisation corpus described in https://arxiv.org/pdf/2208.04083.pdf ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Table of contents"
      ],
      "metadata": {
        "id": "U_Dzawo3k2YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Constructing models for the Task of Microblog Opinion Summarisation](#scrollTo=PGGvuJqQkotL)\n",
        "\n",
        ">>[Table of contents](#scrollTo=U_Dzawo3k2YQ)\n",
        "\n",
        ">>[Prepare your environment](#scrollTo=dKE_mAtIk9u8)\n",
        "\n",
        ">>[Construction of the gold standard summary](#scrollTo=TKXz8EbKOc9g)\n",
        "\n",
        ">>[Summarisation Model and Fine-tuning](#scrollTo=JYEsFM58Oc9i)\n",
        "\n",
        ">>[Generated Summary Evaluation](#scrollTo=RBlNZjywOc9k)"
      ],
      "metadata": {
        "id": "8CrIzbb0k98J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare your environment"
      ],
      "metadata": {
        "id": "dKE_mAtIk9u8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kYrUVlZOc9Z",
        "outputId": "27284ace-d64c-45db-9f5d-756f88a64493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: datasets in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: aiohttp in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: responses<0.19 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: multiprocess in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.23.3)\n",
            "Requirement already satisfied: packaging in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.10.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: sentence-transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.10.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.23.3)\n",
            "Requirement already satisfied: torchvision in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: scipy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.9.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.22.2)\n",
            "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
            "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (4.22.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: rouge-score in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.23.3)\n",
            "Requirement already satisfied: absl-py in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.2.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (2022.9.13)\n",
            "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (4.64.1)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: bert_score in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (0.3.11)\n",
            "Requirement already satisfied: matplotlib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (1.12.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (4.64.1)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (2.28.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: transformers>=3.0.0numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (4.22.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (1.23.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (0.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (2022.9.13)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (4.37.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (9.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (2022.9.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (2.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting tabulate\n",
            "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.8.10\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install -r rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "!pip install bert_score\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQP5eAtOc9c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy\n",
        "import re\n",
        "import shutil\n",
        "import tabulate\n",
        "\n",
        "from transformers import BartForConditionalGeneration, AutoModel, AutoTokenizer, DataCollatorForSeq2Seq \n",
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/mos_workspace/'"
      ],
      "metadata": {
        "id": "9EKsCCLVOrqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt5VUjOOc9d"
      },
      "source": [
        "Most sequence-to-sequence models have input limit of 512/1024 tokens.\n",
        "\n",
        "Order the tweets within a cluster by relevance. \n",
        "We use sentence-tranformers library from Hugging Face to embed the tweets in a vector space and then use the cosine similarity as proxy for a tweet's relevance in the cluster. This can be done in two ways:\n",
        "- <i>Medoid</i>: For each tweet, compute the mean distance to all other tweets in the vector space. Find the tweet (medoid) which minimises this. As an alternative to cosine similarity, one can use Euclidean distance.\n",
        "- <i>Centroid</i>: Compute the centroid of the cluster (which usually does not exists in practice) as the mean average of their embeddings. Then find the closest tweet to the centroid as the centroid approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXkXCBrhOc9d"
      },
      "outputs": [],
      "source": [
        "def cluster_distance(tweet_emb):\n",
        "    distances = [1-util.pytorch_cos_sim(tweet_emb, tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
        "    return sum(distances)\n",
        "\n",
        "def cluster_distance2(tweet_emb):\n",
        "    distances = [numpy.linalg.norm(tweet_emb- tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
        "    return sum(distances)  \n",
        "\n",
        "def centroid_distance(tweet_emb, centroid):\n",
        "    return 1- util.pytorch_cos_sim(tweet_emb, centroid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMLSWLjcOc9e"
      },
      "source": [
        "In our preprocessing, we remove emojis, URL placeholders and whitespace characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVF1SYMpOc9e",
        "outputId": "a5670800-2161-4660-8950-c6f8bf204067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World!😄 \n",
            "What a great day to learn! Check this URL_LINK\n",
            "['Hello World!  What a great day to learn! Check this ']\n"
          ]
        }
      ],
      "source": [
        "#Preprocess tweets\n",
        "def preprocess_(tweets):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    tweets = [emoji_pattern.sub(r'', tweet) for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\n', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\r', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\t', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('URL_LINK','') for tweet in tweets]\n",
        "   \n",
        "    return tweets\n",
        "\n",
        "tweet = 'Hello World!😄 \\nWhat a great day to learn! Check this URL_LINK'\n",
        "print(tweet)\n",
        "print(preprocess_([tweet]))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjhbO2VGOc9g",
        "outputId": "f0c034c1-d4ef-493e-dd2b-f665706cf7a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/config.json\n",
            "Model config MPNetConfig {\n",
            "  \"_name_or_path\": \"/Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/\",\n",
            "  \"architectures\": [\n",
            "    \"MPNetForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"mpnet\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"vocab_size\": 30527\n",
            "}\n",
            "\n",
            "loading weights file /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MPNetModel.\n",
            "\n",
            "All the weights of MPNetModel were initialized from the model checkpoint at /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetModel for predictions without further training.\n",
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most relevant 7 tweets in the cluster are: ['@DisruptedSkies cos apparently a homeless man  presented Miley with an award', \"Also, if Miley hadn't won, dragging along the homeless bloke would have been a waste of time. I'm calling shenanigans. #VMA2014\", \"@elzmoyz she didn't accept her award for video of the year and instead go a homeless young man to accept it on behalf of homeless youths\", '@Mushie_May instead of accepting the award herself her date was a homeless man she helped and he accepted the award in behalf of her charity', 'Miley was less Wrecking Ball and more bawling wreck when she had homeless Jesse accept her VMA award. Watch: ', \"@chl0ehampson Yeah a homeless person collects her award for her doesn't he! And she cries\", 'Nice of #vma2014 miley cyrus to bring along a publicity stunt... i mean homeless person.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "k = 7\n",
        "model_st = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "def sort_relevance(cluster):\n",
        "    tweets = []\n",
        "    minv = 100000 \n",
        "    pp_tweets = preprocess_(cluster['tweets'])\n",
        "    tweet_embeddings = model_st.encode(pp_tweets)\n",
        "    centroid = sum(tweet_embeddings)/len(tweet_embeddings)\n",
        "\n",
        "    for i, tweet in enumerate(pp_tweets):\n",
        "        tweets.append({'tweet':tweet, 'score': cluster_distance(tweet_embeddings[i])})\n",
        "\n",
        "    tweets = sorted(tweets, key=lambda x: x['score'])\n",
        "    return [x['tweet'] for x in tweets]\n",
        "\n",
        "with open(dir_path + '/testing_set/opi_elections/2014-08-25-22_homeless_0') as f:\n",
        "    cluster = json.load(f)\n",
        "\n",
        "print('The most relevant {} tweets in the cluster are:'.format(k), sort_relevance(cluster)[:k])   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKXz8EbKOc9g"
      },
      "source": [
        "# Construction of the gold standard summary\n",
        "\n",
        "Our chosen summary structure diverges from current summarisation approaches that reconstruct the “most popular opinion” (Bražinskas et al., 2020; Angelidis et al., 2021). Instead, we aim to showcase a spectrum of diverse opinions regarding the same event as this is a rich resource for practitioners. Thus, the summary template comprises\n",
        "three components: \n",
        "\n",
        "* **Main Story**: serves to succinctly present the focus of the cluster (often an event)\n",
        "* **Majority Opinion**: the opinion expressed by most posts in the cluster regarding the main story\n",
        "* **Minority Opinion(s)**: opinions expressed by few posts in the cluster regarding the main story. \n",
        "\n",
        "For the purpose of our project, the gold standard summary is the concatenation of all components: main story, majority opinion (if any) and minority opinions (if any). \n",
        "\n",
        "We define a cluster with many opinions as *opinionated* and a cluster with mostly factual information (and few or no opinions) as *non-opinionated*. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5jgHdK1Oc9h",
        "outputId": "212381a0-f131-4679-f43b-ab2d5e97c395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opinionated summaries are 37.8859649122807 tokens long on avg\n",
            "Non-Opinionated summaries are 18.41 tokens long on avg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def summary_concat(cluster):\n",
        "    opinionated = 1\n",
        "    if 'majority_opinion' in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
        "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion'] + ' ' + cluster['minority_opinions']\n",
        "    elif 'majority_opinion' in cluster.keys() and 'minority_opinions' not in cluster.keys():  \n",
        "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion']\n",
        "    elif 'majority_opinion' not in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
        "        summary = cluster['main_story'] + ' ' + cluster['minority_opinions']   \n",
        "    else:\n",
        "       summary = cluster['main_story']  \n",
        "       opinionated = 0\n",
        "    return summary, opinionated        \n",
        "\n",
        "\n",
        "avg_len_opi = 0\n",
        "avg_len_nopi = 0\n",
        "list_clusters = []\n",
        "for file in os.listdir(dir_path + 'Partition_10'):\n",
        "    if not os.path.isfile(os.path.join(dir_path + 'Partition_10',file)):\n",
        "        continue\n",
        "    with open(os.path.join(dir_path + 'Partition_10',file),'r') as f:\n",
        "        cluster = json.load(f)\n",
        "    \n",
        "    list_clusters.append({'text':' '.join(sort_relevance(cluster)), 'summary':summary_concat(cluster)[0]})\n",
        "    \n",
        "    if summary_concat(cluster)[1] == 0:\n",
        "        avg_len_nopi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
        "    \n",
        "    else:\n",
        "        avg_len_opi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
        "\n",
        "avg_len_opi = avg_len_opi/len(os.listdir(dir_path + 'Partition_10/opinionated'))\n",
        "avg_len_nopi = avg_len_nopi/len(os.listdir(dir_path + 'Partition_10/non_opinionated'))\n",
        "\n",
        "with open(dir_path + 'partition10_relevance.json','w') as g:\n",
        "    for cluster in list_clusters:\n",
        "        g.write(json.dumps(cluster)+'\\n')\n",
        "\n",
        "\n",
        "print('Opinionated summaries are {:2} tokens long on avg'.format(avg_len_opi))\n",
        "print('Non-Opinionated summaries are {:2} tokens long on avg'.format(avg_len_nopi))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYEsFM58Oc9i"
      },
      "source": [
        "# Summarisation Model and Fine-tuning\n",
        "\n",
        "We work with Transformer-based model BART for the summarisation task. Its paper can be found here https://arxiv.org/pdf/1910.13461.pdf \n",
        "The model produces competitive results across many tasks and domains and it is especially effective for fine-tuning strategies.\n",
        "We follow the implementation from HuggingFace library and use both its off-the-shelf version as well as fine-tuned version for our task.\n",
        "\n",
        "BART is fine-tuned for 5 epochs on the 10% partition of the original MOS corpus. The data is stored in file *partition10_relevance.json*. The final model will be saved in the folder ./mos_workspace/BART_model.\n",
        "\n",
        "Note that we use the <b>Trainer</b> class for ease of use. 'Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.' More details can be found at https://huggingface.co/docs/transformers/main_classes/trainer .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8J3CJ0POc9i"
      },
      "outputs": [],
      "source": [
        "#Load the off-the-shelf BART model\n",
        "\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#Set the length limits for each summary type: opinionated and non-opinionated clusters.\n",
        "length_limits= {'opi':[12,24],'nopi':[26,40]}\n",
        "\n",
        "def bart_zero(cluster,max_length,min_length):\n",
        "    original_text = ' '.join(sort_relevance(cluster))\n",
        "    inputs = tokenizer.encode(original_text, return_tensors=\"pt\", max_length=1024)\n",
        "    outputs = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True,no_repeat_ngram_size=4)\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "    return summary\n",
        "print(bart_zero(cluster))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"text\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "P2uxx9n3tqUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune the summarisation model on our dataset. Note that running this cell is optional,  a fine-tuned version of the model can be found [here](https://drive.google.com/drive/folders/1cUrIDM4C6vVM1Rha0UJ9Pg6Ds33cUg33?usp=sharing) (you need to download the entire folder you are directed to in the link; download it and put it in your workspace folder.)."
      ],
      "metadata": {
        "id": "cQciMEhpoQ4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC43IWHvOc9j"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir= dir_path + 'BART_model',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir= dir_path + 'BART_model/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "datafiles =dir_path + 'partition10_relevance.json'\n",
        "dataset = load_dataset('json', data_files=datafiles, split='train')\n",
        "tokenized_dataset = dataset.map(preprocess_function,batched=False)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_dataset,         # training dataset\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDtdyw4zOc9j",
        "outputId": "cf5fb224-71e0-4b43-89c7-7582d7cf5fb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "loading file vocab.json\n",
            "loading file merges.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Miley Cyrus asked a homeless man to accept Video of the Year at the VMA Awards on behalf of her charity The majority think it was a good idea for Miley to allow the homeless man to\n"
          ]
        }
      ],
      "source": [
        "model_name_ft = dir_path +'BART_model'\n",
        "model_ft = BartForConditionalGeneration.from_pretrained(model_name_ft)\n",
        "tokenizer_ft = AutoTokenizer.from_pretrained(model_name_ft)\n",
        "\n",
        "\n",
        "def bart_ft(cluster,max_length,min_length):\n",
        "    original_text = ' '.join(sort_relevance(cluster))\n",
        "    inputs = tokenizer_ft.encode(original_text, return_tensors=\"pt\", max_length=1024)\n",
        "    outputs = model_ft.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True,no_repeat_ngram_size=4)\n",
        "\n",
        "    summary = tokenizer_ft.decode(outputs[0],skip_special_tokens=True)\n",
        "    return summary\n",
        "print(bart_ft(cluster))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEEKR3UWtEgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlNZjywOc9k"
      },
      "source": [
        "# Generated Summary Evaluation\n",
        "\n",
        "The generated summaries are evaluated against two dimensions:\n",
        "\n",
        "1. <b>Word-overlap</b>: ROUGE (Lin et al.,2004, https://aclanthology.org/W04-1013.pdf) is the most used metric in summarisation tasks. It calculates the n-gram overlap between system and candidate summaries. In our experiment, we use the harmonic mean F_1 of ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-L (longest sequence). By design, the metric is unable to detect semantic similarity (<i>happy kid</i> and <i>joyful child</i> are identified as disjoint and are penalised).\n",
        "\n",
        "2. <b>Semantic Similarity</b>: BERTScore (Zhang et al, 2020, https://arxiv.org/pdf/1904.09675.pdf) is used in machine generation tasks to detect the level of semantic similarity between a pair of texts. It relies on BERT contextual embeddings and greedy token matching and it has been shown to correlate well with human judgement. It achieved competitive results in machine translation and image captioning due to its robustness in paraphrasing. Note that the metric tends to have a narrow score range between [0.70, 1.00]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTMUX7LOOc9k"
      },
      "outputs": [],
      "source": [
        "# ROUGE & BERTScore Evaluation\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
        "testing_set_path = dir_path + 'testing_set'\n",
        "\n",
        "for partition in ['nopi_covid','nopi_elections','opi_covid','opi_elections']:\n",
        "    rouge1_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    rouge2_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    rougel_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    bertscore_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0} \n",
        "    \n",
        "    for file in os.listdir(os.path.join(testing_set_path,partition)):\n",
        "        if file[0]=='.':\n",
        "            continue\n",
        "        with open(os.path.join(testing_set_path,partition,file),'r') as f:\n",
        "            cluster = json.load(f)\n",
        "    \n",
        "        gold_standard = summary_concat(cluster)[0]\n",
        "        \n",
        "        #Generate the summary candidates\n",
        "        medoid_summary = sort_relevance(cluster)[0]\n",
        "        \n",
        "        min_length = length_limits[partition.split('_')[0]][0]\n",
        "        max_length = length_limits[partition.split('_')[0]][1]\n",
        "        bart_zero_summary = bart_zero(cluster,min_length,max_length)\n",
        "        bart_ft_summary = bart_ft(cluster,min_length,max_length)\n",
        "        summaries = {'medoid':medoid_summary, 'bart_zero':bart_zero_summary, 'bart_ft':bart_ft_summary}\n",
        "        print(summaries)\n",
        "        #For each summary type, compute its scores vs the human-written summary\n",
        "        for key in summaries.keys():\n",
        "        \n",
        "            scores = scorer.score(gold_standard, summaries[key])\n",
        "\n",
        "            rouge1_score[key] += scores['rouge1'][2]\n",
        "            rouge2_score[key] += scores['rouge2'][2]\n",
        "            rougel_score[key] += scores['rougeL'][2]\n",
        "            \n",
        "            P, R, F1 = score([gold_standard], [summaries[key]], lang='en', verbose=False)\n",
        "            bertscore_score[key] += F1\n",
        "    print('Results for partition: ',partition)\n",
        "    data = [['Model','ROUGE-1', 'ROUGE-2','ROUGE-L', 'BERTScore']]\n",
        "    for key in summaries.keys():\n",
        "        data.append([key,rouge1_score[key]/25,rouge2_score[key]/25,rougel_score[key]/25,bertscore_score[key]/25])\n",
        "    table = tabulate.tabulate(data)  \n",
        "    print(table)\n",
        "            \n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b4f355363accec33f6aef60bd5e6eb3b3e1c068e14064fe89a75a68e833dd334"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit ('mos_jupyter': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}