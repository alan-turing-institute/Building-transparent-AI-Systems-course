{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klJ4ampNOc9W"
      },
      "source": [
        "We will use use the Microblog Opinion Summarisation corpus described in https://arxiv.org/pdf/2208.04083.pdf ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kYrUVlZOc9Z",
        "outputId": "27284ace-d64c-45db-9f5d-756f88a64493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: datasets in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: aiohttp in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: responses<0.19 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: multiprocess in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.23.3)\n",
            "Requirement already satisfied: packaging in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (1.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (0.10.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: sentence-transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.10.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.23.3)\n",
            "Requirement already satisfied: torchvision in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: scipy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.9.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.22.2)\n",
            "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
            "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: transformers in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (4.22.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: rouge-score in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.23.3)\n",
            "Requirement already satisfied: absl-py in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.2.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: nltk in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (2022.9.13)\n",
            "Requirement already satisfied: click in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from nltk->rouge-score) (4.64.1)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: bert_score in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (0.3.11)\n",
            "Requirement already satisfied: matplotlib in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (1.12.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (4.64.1)\n",
            "Requirement already satisfied: requests in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (2.28.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: transformers>=3.0.0numpy in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from bert_score) (4.22.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (1.23.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (0.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (2022.9.13)\n",
            "Requirement already satisfied: filelock in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from transformers>=3.0.0numpy->bert_score) (6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (4.37.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (9.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (2022.9.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from requests->bert_score) (2.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting tabulate\n",
            "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.8.10\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install -r rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "!pip install bert_score\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQP5eAtOc9c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy\n",
        "import re\n",
        "import shutil\n",
        "import tabulate\n",
        "\n",
        "from transformers import BartForConditionalGeneration, AutoModel, AutoTokenizer, DataCollatorForSeq2Seq \n",
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/Users/ibilal/Desktop/Research/Code/mos_workspace/'"
      ],
      "metadata": {
        "id": "9EKsCCLVOrqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt5VUjOOc9d"
      },
      "source": [
        "Most sequence-to-sequence models have input limit of 512/1024 tokens.\n",
        "\n",
        "Order the tweets within a cluster by relevance. \n",
        "We use sentence-tranformers library from Hugging Face to embed the tweets in a vector space and then use the cosine similarity as proxy for a tweet's relevance in the cluster. This can be done in two ways:\n",
        "- <i>Medoid</i>: For each tweet, compute the mean distance to all other tweets in the vector space. Find the tweet (medoid) which minimises this. As an alternative to cosine similarity, one can use Euclidean distance.\n",
        "- <i>Centroid</i>: Compute the centroid of the cluster (which usually does not exists in practice) as the mean average of their embeddings. Then find the closest tweet to the centroid as the centroid approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXkXCBrhOc9d"
      },
      "outputs": [],
      "source": [
        "def cluster_distance(tweet_emb):\n",
        "    distances = [1-util.pytorch_cos_sim(tweet_emb, tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
        "    return sum(distances)\n",
        "\n",
        "def cluster_distance2(tweet_emb):\n",
        "    distances = [numpy.linalg.norm(tweet_emb- tweet_emb2) for tweet_emb2 in tweet_embeddings]\n",
        "    return sum(distances)  \n",
        "\n",
        "def centroid_distance(tweet_emb, centroid):\n",
        "    return 1- util.pytorch_cos_sim(tweet_emb, centroid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMLSWLjcOc9e"
      },
      "source": [
        "In our preprocessing, we remove emojis, URL placeholders and whitespace characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVF1SYMpOc9e",
        "outputId": "a5670800-2161-4660-8950-c6f8bf204067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World!ðŸ˜„ \n",
            "What a great day to learn! Check this URL_LINK\n",
            "['Hello World!  What a great day to learn! Check this ']\n"
          ]
        }
      ],
      "source": [
        "#Preprocess tweets\n",
        "def preprocess_(tweets):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    tweets = [emoji_pattern.sub(r'', tweet) for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\n', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\r', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('\\t', ' ') for tweet in tweets]\n",
        "    tweets = [tweet.replace('URL_LINK','') for tweet in tweets]\n",
        "   \n",
        "    return tweets\n",
        "\n",
        "tweet = 'Hello World!ðŸ˜„ \\nWhat a great day to learn! Check this URL_LINK'\n",
        "print(tweet)\n",
        "print(preprocess_([tweet]))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjhbO2VGOc9g",
        "outputId": "f0c034c1-d4ef-493e-dd2b-f665706cf7a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/config.json\n",
            "Model config MPNetConfig {\n",
            "  \"_name_or_path\": \"/Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/\",\n",
            "  \"architectures\": [\n",
            "    \"MPNetForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"mpnet\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"vocab_size\": 30527\n",
            "}\n",
            "\n",
            "loading weights file /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing MPNetModel.\n",
            "\n",
            "All the weights of MPNetModel were initialized from the model checkpoint at /Users/ibilal/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetModel for predictions without further training.\n",
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most relevant 7 tweets in the cluster are: ['@DisruptedSkies cos apparently a homeless man  presented Miley with an award', \"Also, if Miley hadn't won, dragging along the homeless bloke would have been a waste of time. I'm calling shenanigans. #VMA2014\", \"@elzmoyz she didn't accept her award for video of the year and instead go a homeless young man to accept it on behalf of homeless youths\", '@Mushie_May instead of accepting the award herself her date was a homeless man she helped and he accepted the award in behalf of her charity', 'Miley was less Wrecking Ball and more bawling wreck when she had homeless Jesse accept her VMA award. Watch: ', \"@chl0ehampson Yeah a homeless person collects her award for her doesn't he! And she cries\", 'Nice of #vma2014 miley cyrus to bring along a publicity stunt... i mean homeless person.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "k = 7\n",
        "model_st = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "def sort_relevance(cluster):\n",
        "    tweets = []\n",
        "    minv = 100000 \n",
        "    pp_tweets = preprocess_(cluster['tweets'])\n",
        "    tweet_embeddings = model_st.encode(pp_tweets)\n",
        "    centroid = sum(tweet_embeddings)/len(tweet_embeddings)\n",
        "\n",
        "    for i, tweet in enumerate(pp_tweets):\n",
        "        tweets.append({'tweet':tweet, 'score': cluster_distance(tweet_embeddings[i])})\n",
        "\n",
        "    tweets = sorted(tweets, key=lambda x: x['score'])\n",
        "    return [x['tweet'] for x in tweets]\n",
        "\n",
        "with open(dir_path + '/testing_set/opi_elections/2014-08-25-22_homeless_0') as f:\n",
        "    cluster = json.load(f)\n",
        "\n",
        "print('The most relevant {} tweets in the cluster are:'.format(k), sort_relevance(cluster)[:k])   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKXz8EbKOc9g"
      },
      "source": [
        "# Summary Generation\n",
        "\n",
        "We work with Transformer-based model BART for the summarisation task. Its paper can be found here https://arxiv.org/pdf/1910.13461.pdf \n",
        "The model produces competitive results across many tasks and domains and it is especially effective for fine-tuning strategies.\n",
        "We follow the implementation from HuggingFace library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5jgHdK1Oc9h",
        "outputId": "212381a0-f131-4679-f43b-ab2d5e97c395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opinionated summaries are 37.8859649122807 tokens long on avg\n",
            "Non-Opinionated summaries are 18.41 tokens long on avg\n"
          ]
        }
      ],
      "source": [
        "# The final summary is the concatenation of the components: main story, majority opinion (if any) and any minority opinions (if any). \n",
        "def summary_concat(cluster):\n",
        "    opinionated = 1\n",
        "    if 'majority_opinion' in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
        "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion'] + ' ' + cluster['minority_opinions']\n",
        "    elif 'majority_opinion' in cluster.keys() and 'minority_opinions' not in cluster.keys():  \n",
        "        summary = cluster['main_story'] + ' ' + cluster['majority_opinion']\n",
        "    elif 'majority_opinion' not in cluster.keys() and 'minority_opinions' in cluster.keys():\n",
        "        summary = cluster['main_story'] + ' ' + cluster['minority_opinions']   \n",
        "    else:\n",
        "       summary = cluster['main_story']  \n",
        "       opinionated = 0\n",
        "    return summary, opinionated        \n",
        "\n",
        "\n",
        "avg_len_opi = 0\n",
        "avg_len_nopi = 0\n",
        "list_clusters = []\n",
        "for file in os.listdir(dir_path + 'Partition_10'):\n",
        "    if not os.path.isfile(os.path.join(dir_path + 'Partition_10',file)):\n",
        "        continue\n",
        "    with open(os.path.join(dir_path + 'Partition_10',file),'r') as f:\n",
        "        cluster = json.load(f)\n",
        "    \n",
        "    list_clusters.append({'text':' '.join(sort_relevance(cluster)), 'summary':summary_concat(cluster)[0]})\n",
        "    \n",
        "    if summary_concat(cluster)[1] == 0:\n",
        "        avg_len_nopi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
        "    \n",
        "    else:\n",
        "        avg_len_opi += len(word_tokenize(summary_concat(cluster)[0]))  \n",
        "\n",
        "avg_len_opi = avg_len_opi/len(os.listdir(dir_path + 'Partition_10/opinionated'))\n",
        "avg_len_nopi = avg_len_nopi/len(os.listdir(dir_path + 'Partition_10/non_opinionated'))\n",
        "\n",
        "with open(dir_path + 'partition10_relevance.json','w') as g:\n",
        "    for cluster in list_clusters:\n",
        "        g.write(json.dumps(cluster)+'\\n')\n",
        "\n",
        "\n",
        "print('Opinionated summaries are {:2} tokens long on avg'.format(avg_len_opi))\n",
        "print('Non-Opinionated summaries are {:2} tokens long on avg'.format(avg_len_nopi))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8J3CJ0POc9i",
        "outputId": "9329a79d-e553-403e-a35d-6b4c6fc0dc04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
            "Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/vocab.json\n",
            "loading file merges.txt from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/merges.txt\n",
            "loading file tokenizer.json from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /Users/ibilal/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@DisruptedSkies cos apparently a homeless man  presented Miley with an award Also, if Miley hadn't won, dragging along the homeless bloke would have been a waste of time. #Croc\n"
          ]
        }
      ],
      "source": [
        "model_name = 'facebook/bart-large-cnn'\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#Set the length limits for each summary type: opinionated and non-opinionated clusters.\n",
        "length_limits= {'opi':[12,24],'nopi':[26,40]}\n",
        "\n",
        "def bart_zero(cluster,max_length,min_length):\n",
        "    original_text = ' '.join(sort_relevance(cluster))\n",
        "    inputs = tokenizer.encode(original_text, return_tensors=\"pt\", max_length=1024)\n",
        "    outputs = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True,no_repeat_ngram_size=4)\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "    return summary\n",
        "print(bart_zero(cluster))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYEsFM58Oc9i"
      },
      "source": [
        "# Model Fine-tuning\n",
        "\n",
        "BART is fine-tuned for 5 epochs on the 10% partition of the original MOS corpus. The data is stored in file partition10_relevance.json. The final model will be saved in the folder ./mos_workspace/BART_model.\n",
        "\n",
        "Note that we use the <b>Trainer</b> class for ease of use. 'Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.' More details can be found at https://huggingface.co/docs/transformers/main_classes/trainer .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7b58c9f96ef348ef9849961690ba7830",
            "8d0c7f6508f14223b71b7cfc3712a86e",
            "ff92df9677c5462aa21f087c5d657577",
            "d89e33548d37429193f98587444a0254",
            "a878a5cf4fbf4f62a05640b746afdc4e"
          ]
        },
        "id": "MC43IWHvOc9j",
        "outputId": "9c0d3318-2afd-4603-a750-ce0ce138b702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using custom data configuration default-a3fec9c522116920\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default to /Users/ibilal/.cache/huggingface/datasets/json/default-a3fec9c522116920/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b58c9f96ef348ef9849961690ba7830",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d0c7f6508f14223b71b7cfc3712a86e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff92df9677c5462aa21f087c5d657577",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 tables [00:00, ? tables/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to /Users/ibilal/.cache/huggingface/datasets/json/default-a3fec9c522116920/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d89e33548d37429193f98587444a0254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/214 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ibilal/opt/anaconda3/envs/mos_jupyter/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 214\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 135\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a878a5cf4fbf4f62a05640b746afdc4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/135 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, text. If summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.3316, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.37}\n",
            "{'loss': 1.3068, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.74}\n",
            "{'loss': 1.3658, 'learning_rate': 3e-06, 'epoch': 1.11}\n",
            "{'loss': 1.3004, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.48}\n",
            "{'loss': 1.2155, 'learning_rate': 5e-06, 'epoch': 1.85}\n",
            "{'loss': 1.1346, 'learning_rate': 6e-06, 'epoch': 2.22}\n",
            "{'loss': 1.1019, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.59}\n",
            "{'loss': 1.0527, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.96}\n",
            "{'loss': 0.925, 'learning_rate': 9e-06, 'epoch': 3.33}\n",
            "{'loss': 0.8982, 'learning_rate': 1e-05, 'epoch': 3.7}\n",
            "{'loss': 0.8612, 'learning_rate': 1.1000000000000001e-05, 'epoch': 4.07}\n",
            "{'loss': 0.6686, 'learning_rate': 1.2e-05, 'epoch': 4.44}\n",
            "{'loss': 0.6978, 'learning_rate': 1.3000000000000001e-05, 'epoch': 4.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model\n",
            "Configuration saved in /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 16268.5589, 'train_samples_per_second': 0.066, 'train_steps_per_second': 0.008, 'train_loss': 1.0578421663354944, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/pytorch_model.bin\n",
            "tokenizer config file saved in /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/tokenizer_config.json\n",
            "Special tokens file saved in /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"text\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= dir_path + 'BART_model',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir= dir_path + 'BART_model/logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n",
        "datafiles =dir_path + 'partition10_relevance.json'\n",
        "dataset = load_dataset('json', data_files=datafiles, split='train')\n",
        "tokenized_dataset = dataset.map(preprocess_function,batched=False)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_dataset,         # training dataset\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDtdyw4zOc9j",
        "outputId": "cf5fb224-71e0-4b43-89c7-7582d7cf5fb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /Users/ibilal/Desktop/Research/Code/mos_workspace/BART_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "loading file vocab.json\n",
            "loading file merges.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Miley Cyrus asked a homeless man to accept Video of the Year at the VMA Awards on behalf of her charity The majority think it was a good idea for Miley to allow the homeless man to\n"
          ]
        }
      ],
      "source": [
        "model_name_ft = dir_path +'BART_model'\n",
        "model_ft = BartForConditionalGeneration.from_pretrained(model_name_ft)\n",
        "tokenizer_ft = AutoTokenizer.from_pretrained(model_name_ft)\n",
        "\n",
        "\n",
        "def bart_ft(cluster,max_length,min_length):\n",
        "    original_text = ' '.join(sort_relevance(cluster))\n",
        "    inputs = tokenizer_ft.encode(original_text, return_tensors=\"pt\", max_length=1024)\n",
        "    outputs = model_ft.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True,no_repeat_ngram_size=4)\n",
        "\n",
        "    summary = tokenizer_ft.decode(outputs[0],skip_special_tokens=True)\n",
        "    return summary\n",
        "print(bart_ft(cluster))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlNZjywOc9k"
      },
      "source": [
        "# Summary Evaluation\n",
        "\n",
        "Generated summaries are evaluated against two dimensions:\n",
        "\n",
        "1. <b>Word-overlap</b>: ROUGE (Lin et al.,2004, https://aclanthology.org/W04-1013.pdf) is the most used metric in summarisation tasks. It calculates the n-gram overlap between system and candidate summaries. In our experiment, we use the harmonic mean F_1 of ROUGE-1 (unigrams), ROUGE-2 (bigrams) and ROUGE-L (longest sequence). By design, the metric is unable to detect semantic similarity (<i>happy kid</i> and <i>joyful child</i> are identified as disjoint and are penalised).\n",
        "\n",
        "2. <b>Semantic Similarity</b>: BERTScore (Zhang et al, 2020, https://arxiv.org/pdf/1904.09675.pdf) is used in machine generation tasks to detect the level of semantic similarity between a pair of texts. It relies on BERT contextual embeddings and greedy token matching and it has been shown to correlate well with human judgement. It achieved competitive results in machine translation and image captioning due to its robustness in paraphrasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTMUX7LOOc9k"
      },
      "outputs": [],
      "source": [
        "# ROUGE Evaluation\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=True)\n",
        "testing_set_path = dir_path + 'testing_set'\n",
        "\n",
        "for partition in ['nopi_covid','nopi_elections','opi_covid','opi_elections']:\n",
        "    rouge1_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    rouge2_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    rougel_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0}\n",
        "    bertscore_score = {'medoid': 0, 'bart_zero':0, 'bart_ft':0} \n",
        "    \n",
        "    for file in os.listdir(os.path.join(testing_set_path,partition)):\n",
        "        if file[0]=='.':\n",
        "            continue\n",
        "        with open(os.path.join(testing_set_path,partition,file),'r') as f:\n",
        "            cluster = json.load(f)\n",
        "    \n",
        "        gold_standard = summary_concat(cluster)[0]\n",
        "        \n",
        "        #Generate the summary candidates\n",
        "        medoid_summary = sort_relevance(cluster)[0]\n",
        "        \n",
        "        min_length = length_limits[partition.split('_')[0]][0]\n",
        "        max_length = length_limits[partition.split('_')[0]][1]\n",
        "        bart_zero_summary = bart_zero(cluster,min_length,max_length)\n",
        "        bart_ft_summary = bart_ft(cluster,min_length,max_length)\n",
        "        summaries = {'medoid':medoid_summary, 'bart_zero':bart_zero_summary, 'bart_ft':bart_ft_summary}\n",
        "        print(summaries)\n",
        "        #For each summary type, compute its scores vs the human-written summary\n",
        "        for key in summaries.keys():\n",
        "        \n",
        "            scores = scorer.score(gold_standard, summaries[key])\n",
        "\n",
        "            rouge1_score[key] += scores['rouge1'][2]\n",
        "            rouge2_score[key] += scores['rouge2'][2]\n",
        "            rougel_score[key] += scores['rougeL'][2]\n",
        "            \n",
        "            P, R, F1 = score([gold_standard], [summaries[key]], lang='en', verbose=False)\n",
        "            bertscore_score[key] += F1\n",
        "    print('Results for partition: ',partition)\n",
        "    data = [['Model','ROUGE-1', 'ROUGE-2','ROUGE-L', 'BERTScore']]\n",
        "    for key in summaries.keys():\n",
        "        data.append([key,rouge1_score[key]/25,rouge2_score[key]/25,rougel_score[key]/25,bertscore_score[key]/25])\n",
        "    table = tabulate.tabulate(data)  \n",
        "    print(table)\n",
        "            \n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b4f355363accec33f6aef60bd5e6eb3b3e1c068e14064fe89a75a68e833dd334"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit ('mos_jupyter': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}